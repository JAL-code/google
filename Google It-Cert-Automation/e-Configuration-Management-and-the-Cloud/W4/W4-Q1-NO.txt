[*Caption:"Intro to Module 4: Managing Cloud Instances at Scale":01*]

[MUSIC] Welcome back.
And guess what?
This is the last module in the course.
Congratulations on making it here.
It's sure been an exciting ride and it's about to get even more interesting.
As we've learned in the past modules Cloud providers offer us a bunch of different
services.
By now we've learned a lot about different hosting models.
If you're working for a small company you might get by with
pre-packaged applications offered in the software as a service model.
But as the organization grows so to do the IT needs.
Eventually, your company might grow so large you need to start developing your
own applications based on the platforms and
infrastructure models from Cloud providers.
[Why might you develop your own application?]
Well developing your own apps [gives your IT team more control and flexibility
over what the applications] do but it also brings a new set of challenges.
You'll [need to figure out how the different pieces fit together, make sure
that the services run reliably and troubleshoot problems when they come up.]
In the next few videos, we'll check out some of the [options for
building software for the Cloud.]
We'll look into the different [types of storage available and
how to decide which one to use.]
We'll also learn more about[ load balancing and
how to distribute a service across many instances.]
We'll discuss how we can [make changes to our systems without breaking everything.]
And we'll check out some of the [limitations that you might run into when
running software in the Cloud.]
We'll wrap up with some [best practices for running reliable services.]
This includes measuring how your service is doing by using a monitoring system and
also setting up alerts so
you're automatically notified if things don't go as planned.
Sometimes systems fail and that's okay.
Using the tips you'll learn in this module you'll be prepared to deal with failure
and troubleshoot issues when using Cloud services.
Once we're done you'll get the chance to debug and
fix a problem with an application running in the Cloud.
Exciting stuff, right?
There's a lot more ground to cover so let's dive in. 

[*Caption:"Storing Data in the Cloud":02*]

Almost all IT systems need to store some data.
Sometimes, it's a lot of data,
sometimes, it's only bits and pieces of information.
Cloud providers give us a lot of storage options.
Picking the right solution for
data storage will depend on what service you're building.
You'll need to consider a bunch of factors, like;
how much data you want to store,
what kind of data that is,
what geographical locations you'll be using it in,
whether you're mostly writing or reading the data,
how often the data changes,
or what your budget is.
This might sound like a lot of things to consider,
but don't worry, it's not that bad.
We'll check out some of
the most common solutions offered by
Cloud providers to give you
a better idea of when to choose what.
[When choosing a storage solution in the Cloud,
you might opt to go with
the traditional storage technologies,
like block storage, or you can choose newer technologies,
like object or blob storage.]
Let's check out what each of these mean.
As we saw in an earlier video,
when we create a VM running in the Cloud,
it has a local disk attached to it.
These local disks are an example of block storage.
This type of storage closely
resembles the physical storage that
you have on physical machines using physical hard drives.
[Block storage in the Cloud acts
almost exactly like a hard drive.
The operating system of
the virtual machine will create and manage a file system
on top of the block storage
just as if it were a physical drive.]
There's a pretty cool difference though.
[These are virtual disks,
so we can easily move the data around.]
For example, [we can migrate
the information on the disk to a different location,
attach the same disk image to other machines,
or create snapshots of the current state.]
All of this without having to ship
a physical device from place to place.
Our block storage can be either persistent or ephemeral.
[Persistent storage is used
for instances that are long lived,
and need to keep data across reboots and upgrades.]
[On the flip side, ephemeral storage is
used for instances that are only temporary,
and only need to keep local data while they're running.]
Ephemeral storage is great for
temporary files that your service
needs to create while it's running,
but you don't need to keep.
This type of storage is especially
common when using containers,
but it can also be useful when dealing with
virtual machines that only
need to store data while they're running.
[In typical Cloud setups,
each VM has one or more disks attached to the machine.
The data on these disks is managed by the OS
and can't be easily shared with other VMs.]
If you're looking [to share data across instances,
you might want to look into
some *shared file system solutions,*
that Cloud providers offer using
the platform as a service model.]
When using these solutions,
the [data can be accessed through
network file system protocols like NFS or CIFS.]
This lets you [connect many different
instances or containers to
the same file system with no programming required.]
[Block storage and shared file systems work fine
when you're managing servers that need to access files.]
But if you're trying to deploy a Cloud app that
needs to [store application data,
you'll probably need to look into
other solutions like objects storage,
which is also known as blob storage.]
[Object storage lets you place in
retrieve objects in a storage bucket.]
These objects are just [generic files
like photos or cat videos,
encoded and stored on disk as binary data.]
[These files are commonly called blobs,
which comes from *binary large object,*
and as we called out,
these blobs are stored in locations known as *buckets.]
[Everything that you put into
a storage bucket has a unique name.]
There's [no file system.]
You place an object into storage with a name,
and if you [want that object back,
you simply ask for it by name.]
To interact with an object store,
you [need to use an API or special utilities that
can interact with the specific
object store that you're using.]
On top of this, we've called out in earlier videos that
[most Cloud providers offer databases as a service.]
These come in two basic flavors, [SQL and NoSQL.]
[SQL databases, also known as relational,
use the traditional database format and query language.]
Data is [stored in tables with
columns and rows that can be indexed,
and we retrieve the data by writing SQL queries.]
A lot of existing applications already use this model,
so it's [typically chosen when migrating
an existing application to the Cloud.]
[NoSQL databases offer a lot
of advantages related to scale.]
They're designed to be [distributed across tons of
machines and are super fast when retrieving results.]
But instead of a unified query language,
we need [to use a specific API provided by the database.]
This means that we might [need to rewrite the portion
of the application that accesses the DB.]
When deciding how to store your data,
[you'll also have to choose a *storage class*.]
Cloud providers typically offer
different classes of storage at different prices.
[Variables like performance, availability,
or how often the data is accessed
will affect the monthly price.]
The performance of a storage solution
is influenced by a number of factors,
including [throughput, IOPS, and latency.]
Let's check out what these mean.
[Throughput is the amount of data that you can
read and write in a given amount of time.]
The throughput for reading and
writing can be pretty different.
For example, you could have
a [throughput of one gigabyte per
second for reading and
100 megabytes per second for writing.]
[IOPS or input/output operations
per second measures how many reads
or writes you can do in one second,
no matter how much data you're accessing.]
[Each read or write operation has some overhead.]
So there's a limit on how many
you can do in a given second,
and [latency is the amount of
time it takes to complete a read or write operation.]
This will t[ake into account the impact of IOPS,
throughput and the particulars of the specific service.]
[Read latency is sometimes
reported as the time it takes a storage system
to start delivering data
after a read request has been made,
also known as *time to first byte.*]
While [write latency is typically measured as the amount
of time it takes for a write operation to complete.]
When [choosing the storage class to use,
you might come across terms like hot and cold.]
[Hot data is accessed frequently and stored in
hot storage] while [cold data is accessed infrequently,
and stored in cold storage.]
These two storage types
have different performance characteristics.
For example, [hot storage back ends
are usually built using solid state disks,
which are generally faster than
the traditional spinning hard disks.]
So how do you choose between one and the other?
Say you want to keep all the data you're
service produces for five years,
but you don't expect to regularly
access data older than one year.
You might [choose to keep the last one year of data in
hot storage] so you have fast access to it,
and [after a year,
you can move your data to
cold storage where you can still get to it,
but it will be slower and possibly costs more to access.]
There's a lot more to say about storage in the Cloud.
It's really quite a hot topic,
but we won't go into more detail here.
We'll provide links to more info in
the next reading in case you want to learn more.
Up next, we're going to look at
a different Cloud scale characteristic that we've
already touched on using
multiple machines for the same service. 

[*Caption:"Load Balancing":03*]

[MUSIC]
In earlier videos, we saw a bunch of different reasons why
we might [want more than one machine or container running our service.]
For example, we might want to[ horizontally scale our service to handle more work,
distribute instances geographically to get closer to our users.]
Or have [backup instances to keep the service running if one or
more of the instances fail.]
No matter the reason, we [use orchestration tools and
techniques to make sure that the instances are repeatable.]
And once we've set up [replicated machines,
we'll want to distribute the requests across instances.]
We called out earlier that this is where [load balancing comes into play.]
Let's take a closer look at the different load balancing methods that we can use.
A pretty common load balancing technique is [round robin DNS.]
[Round robin is a really common method for distributing tasks.]
Imagine you're giving out treats at a party.
First, you make sure that each of your friends [gets one cookie.]
Then you [give everyone a second serving ]and so
on [until all of the treats are gone] or [your guests say, thank you, they're full].
That's the round-robin approach to eating all the cookies.
Now, if we want to [translate a URL like my service.example.com into an IP address,
we use the DNS protocol or domain name system.]
[Clients are served in turn.]
In the simplest configuration,
[he URL always gets translated into exactly the same IP address.]
But when we configure our [DNS to use round robin, it'll give each client asking for
the translation a group of IP addresses in a different order.]
The [clients will then pick one of the addresses] to try to reach the service.
[If an attempt fails, the client will jump to another address] on the list.
This load balancing method is super easy to set up.
You just need to make sure that the IPs of all machines in the pool
are configured in your DNS server, but it has some limitations.
[First, you can't control which addresses get picked by the clients.]
Even if a server is overloaded, you [can't stop the clients from reaching out to it.]
On top of that, [DNS records are cached by the clients and other servers.]
So if you need to [change the list of addresses for the instances, you'll have
to wait until all of the DNS records that were cached by the clients expire.]
There's got to be a better way, right?
Well, there sure is.
To have more control over how the load's distributed and
to make faster changes, [we can set up a server as a dedicated load balancer.]
This is a [machine that acts as a proxy between the clients and the servers.]
It receives the requests and based on the rules that we provide,
it [directs them to the selected back-end server.]
Load balances can be super simple or super complex depending on the service needs.
[Say your service needs to keep track of the actions that a user has taken
up till now.
In this case, you'll want your load balancer to use sticky sessions.]
[Using sticky sessions means all requests from the same client
always go to the same back end server.]
This can be really useful for services than need it but
can also cause headaches when migrating or maintaining your service.
So you need to [use it only if you really need it.
Otherwise, you'll end up in a really sticky situation.]
Another cool feature of [load balancers is that you can configure
them to check the health of the backend servers.]
Typically, we do this by making a simple query to the servers and
checking that the reply matches the expected reply.
[If a back-end server is unhealthy, the load balancer will stop sending new
requests to it to keep only healthy servers in the pool.]
As we've called out a few times already,
a [cool feature of cloud infrastructure is how easily we can add or
remove machines from a pool of servers providing a service.]
If we have a load balancer controlling the load of the machines,
[adding a new machine to the pool is as easy as creating the instance.
And then letting the load balancer know that it can now route traffic to it.]
We can do this by [manually creating and adding the instance or
when our services under heavy load, we can just let the auto scaling feature do it.]
Cool, right?
Okay, so imagine that you've built out your service with load balancers and
you're receiving requests from all over the world.
How do you [make sure that clients connect to the servers that are closest to them?]
You can [use Geo DNS and GeoIP.]
These are DNS configurations that will direct your clients to
the closest geographical load balancer.
The mechanism used to route the traffic relies on how the DNS servers respond to
requests.
For example, from machines hosted in North America, a DNS server in North America
might be configured to respond with the IPs in, you guessed it, North America.
It can be tricky to set this up on your own but
[most Cloud providers offer it as part of their services making
it much easier to have a geographically distributed service.]
Let's take this one step further.
[There are some providers dedicated to bringing the contents of your services
as close to the user as possible.
These are the content delivery networks or CDNs.]
They make up a [network of physical hosts that are geographically located as close
to the end user as possible.]
This means that CDN servers are [often in the same data center
as the users Internet service provider.]
CDNs work by [caching content super close to the user.]
[When a user requests say, a cute cat video,
it's stored in the closest CDN server.
That way, when a second user in the same region requests the same cat video, it's
already cached in a server that's pretty close and it can be downloaded extra fast.]
Because no one should have to wait for their cat videos to load.
You now know a lot of stuff about load-balancing.
Up next, we'll talk about another aspect of dealing with cloud services.
How to deploy changes safely to our cloud service? 

[*Caption:"Change Management":04*]

You've come a long way.
You now know how to get your service running in the cloud.
Next, let's talk about [how to keep it running.]
Most of the time when something stops working, it's because something changed.
If we want our cloud service to be stable,
we might be tempted to avoid changes altogether.
But change is a fact of cloud life.
If we want to fix bugs and
improve features in our services, we have to make changes.
[But we can make changes in a controlled and safe way.
This is called change management, and
it's what lets us keep innovating while our services keep running.]
[Step one in improving the safety of our changes,
we have to make sure they're well-tested.]
This means [running unit tests and integration tests, and
then running these tests whenever there's a change.]
In an earlier course, we briefly mentioned [continuous integration, or
CI,] but here's a refresher.
[A continuous integration system will build and
test our code every time there's a change.]
Ideally, the CI system [runs even for changes that are being reviewed.]
That way you can catch problems [before they're merged into the main branch.]
You can use a common [open source CI system like Jenkins, or
if you use GitHub, you can use its Travis CI integration.]
Many cloud providers also offer continuous integration as a service.
Once the change has committed, the CI system will build and
test the resulting code.
[CI means the software is built, uploaded and tested constantly.]
[Now you can use continuous deployment, or
CD, to automatically deploy the results of the build or build artifacts.]
[Continuous deployment lets you control the deployment with rules.]
For example, we usually [configure our CD system to deploy new builds
only when all of the tests have passed successfully.]
On top of that, we can [configure our CD to push to different environments based on
some rules.]
What do we mean by that?
we [should have a test environment separate from the production environment.]
Having them separate lets us [validate that changes work correctly before they affect
users.]
Here [environment means everything needed to run the service.]
It [includes the machines and networks used for
running the service, the deployed code, the configuration management,
the application configurations, and the customer data.]
[Production, usually shortened to prod, is the real environment,
the ones users see and interact with.]
Because of this, we have to [protect, love, and nurture a prod.]
[The test environment needs to be similar enough to prod that we can use them
to check our changes work correctly.]
You [could have your CD system configured to push new changes to the test
environment.]
You can then [check that the service is still working correctly there, and
then manually tell your deployment system to push those same changes to production.]
If the service is complex and there are a bunch of different developers making
changes to it, you might set up additional environments where the developers can test
their changes in different stages before releasing them.
[For example, you might have your CD system push all new changes to a development or
dev environment, then have a separate environment called pre-prod,
which only gets specific changes after approval.]
[And only after a thorough testing, these changes get pushed to pro.]
Say you're trying to increase the efficiency of your surface by 20%,
but you don't know if the change you made might crash part of your system.
[You want to deploy it to one of those testing or development
environments to make sure it works correctly before you ship it to prod.]
Remember, [these environments need to be as similar to prod as possible.]
They should be [built and deployed in the same way.]
And while we don't want them to be breaking all the time, it's normal for
some changes to break dev or even pre-prod.
We're just happy that we can catch them early so that they don't break prod.
Sometimes you might want to experiment with a new service feature.
You've tested the code, you know it works, but
you want to know if it's something that's going to work well for your users.
When you have something that you want to test in production with real customers,
you can experiment using A/B testing.
[In A/B testing, some requests are served using one set of code and configuration,
A, and other requests are served using a different set of of code and
configuration, B.]
This is another place [where a load balancer and
instance groups can help us out.]
You can [deploy one instance group in your A configuration and
a second instance group in your B configuration.
Then by changing the configuration of the load balancer, you can direct
different percentages of inbound requests to those two configurations.]
If your A configuration is today's production configuration and
your B configuration is something experimental,
you might want to [start by only directing 1 % of your requests to B.]
Then you can slowly ramp up the percentage that you check out whether the B
configuration performs better than A, or not.
[Heads up, make sure you have basic monitoring so
that it's easy to tell if A or B is performing better or worse.]
If it's hard to identify the back-end responsible for serving A requests or
B requests, then much of the [value of A/B testing is lost to A/B debugging.]
So what happens if all the precautions we took aren't enough and
we break something in production?
[Remember what we discussed in an earlier course about post-mortems.
We learn from failure and we build the new knowledge into our change management.]
Ask yourself, what did I have to do to catch the problem?
[Can I have one of my change management systems look for
problems like that in the future?
Can I add a test or a rule to my unit tests, my CI/CD system, or
my service health checks to prevent this kind of failure in the future?]
So remember, if something breaks, give yourself a break.
Sometimes in IT, these things happen, no matter how careful you are.
And as you use and refine your change management systems and skills,
you'll gain the confidence to make changes to your service more quickly and safely. 

[*Caption:"Understanding Limitations":05*]

We've spent a while talking about how to
make your service runs smoothly in the Cloud,
now let's take a moment to talk about
some of the problems that you might come across.
Personally, I find that when
writing software to run on the Cloud,
it's important to keep in mind how
my application will be deployed.
The software I'm creating needs to be fault
tolerant and capable of handling unexpected events.
[Instances might be added or removed from the pool as
needed and if an individual machine crashes,
my service needs to breeze
along without introducing problems,
and not every problem results in a crash,
sometimes we run into quotas or limits,
meaning that you can only perform a certain number of
operations within a certain time period.]
[For example, when using Blob Storage there might be
a limit of 1,000 writes to
the same blob in a given seconds.]
If your service performs a lot
of these operations routinely,
it might get blocked by these limits.
In that case, you'll need to see if you can
change the way you're doing the operations,
for example by grouping all of the calls into one batch.
Switching to a different service
is sometimes an option too.
Some API calls used in
Cloud services can be expensive to perform,
so most Cloud providers will enforce rate limits on
these calls to prevent
one service from overloading the whole system.
For example, there might be a rate limit of
one call per second for an expensive API call.
[On top of that, there are also utilization limits,
which cap the total amount of
a certain resource that you can provision.]
These [quotas are there to help you avoid
unintentionally allocating more
resources than you wanted.]
Imagine you've configured your service
to use auto scaling
and it suddenly receives a huge spike in traffic.
This could mean a lot of new instances getting
deployed which can cost a lot of money.
For some of these limits, [you can
ask for a quota increase
from the Cloud provider if you want additional capacity,]
and you can also set a smaller quota
in the default to avoid overspending.
This can be a great idea when you're
running a service on a tight budget.
[If your service performance
expensive operations routinely,
you should make sure you understand
the limitations of the solution that you choose.]
A lot of platform as a service and infrastructure as
a service offerings have costs
directly related to how much they're used.
They also have usage quotas.
If the service you've built
suddenly becomes very popular,
you can run out of quota or run out of budget.
By imposing a [quota on an auto-scaling system,
the system will grow to meet
user demand until it reaches the configured limit.]
The trick here is to [have
good monitoring and alerting around behavior like this.]
If your system runs out of quota but there's
an increased demand for a puppy videos,
the system may have problems,
degraded performance or worse yet an outage.
So you want to be notified as soon as it happens that you
can decide whether to increase your quota or not.
Finally, let's talk about [dependencies.]
When [your service depends on
a Platform as a Service offering]
like a [hosted database or CICD system,
you're handing the responsibility
for maintenance and upgrades of
that service off to your Cloud provider, that's great,
fewer things to worry about and maintain,]
but it also means that you [don't always get to choose
what version of that software you're using.]
You might find yourself on
[either side of the upgrade cycle,]
either wanting to [stay at a version
that's working well] for you or wanting
the Cloud provider to hurry up and upgrade to
resolve a bug that's affecting your service.
Your Cloud provider has a [strong incentive to
keep its service software fairly up-to-date.]
Keeping software as a service solutions up to date
ensures that [customers aren't
vulnerable to security flaws,]
that [bugs are promptly fixed]
and that [new features get released early.]
At the same time, the [Cloud provider
has to move carefully
and test changes to keep
destruction of its service to a minimum.]
They will [communicate proactively about changes to
the services that you use] and in some cases,
Cloud providers might give you access
to early versions of these services.
[For example, you can set up
a test environment for your service that uses
the beta or prerelease version of
a given software as a service solution,
letting you test it before it impacts production.]
Hopefully, you're starting to get
an idea of the trade-offs that you'll
need to make to get the most from
deploying your software to the Cloud.
Now, head on over to the reading for
links to additional information on all of this,
then there's a quick quiz for you. 

[*Caption:"More About Cloud Providers":06*]

More About Cloud Providers

Here are some links to some common Quotas you’ll find in various cloud providers

    [https://cloud.google.com/compute/quotas#understanding_vm_cpu_and_ip_address_quotas]
    [https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html]
    [https://docs.microsoft.com/en-us/azure/azure-subscription-service-limits#service-specific-limits]

[*Caption:"The End":07*]
