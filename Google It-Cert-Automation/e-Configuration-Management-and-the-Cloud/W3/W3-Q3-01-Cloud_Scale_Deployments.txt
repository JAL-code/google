[A load balancer ensures that each node receives a balanced number of requests. When a request comes in, the load balancer picks a node to serve the response.]
[The simplest one is just to give each node one request called round robin.]
[autoscaling. It allows the service to increase or reduce capacity as needed while the service owner only pays for the cost of the machines that are in use at any given time.]
[To check out how this works in practice, let's look at an example of a web application with a lot of users.]
[This IP address identifies a specific computer, the entry point for the sites.]
[For large applications where speed and availability matter, there will be a couple of layers in between the entry point and the actual web service.]
[The first layer will be a pool of web caching servers with a load balancer to distribute the requests among them.]
[One of the most popular applications for this caching is called Varnish, but of course it's not the only one.]
[The Nginx web server and software also includes this caching functionality.]
[There's a bunch of providers that do web caching as a service like Cloudflare and Fastly.]
[This configured backend is the actual web service that generates the webpages for the site, and it will also normally be a pool of nodes running under a load balancer.]
[But because getting data from a database can be slow, there's usually an extra layer of caching, specific for the database contents.]
[The most popular applications for this level of caching are Memcached and Redis.]
[The infrastructure will take care of adding and removing instances, distributing the load, making sure that each geographical region has the right capacity,]
