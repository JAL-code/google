[The latency is the delay between sending a byte of data from one point and receiving it on the other. This value is directly affected by the physical distance between the two points and how many intermediate devices there are between them.]
[This is effectively the data capacity of the connection.]
[If the web server is hosted somewhere across the ocean, the latency might be a 100 milliseconds or so. That's the time it takes for your request to reach the server. The server will then generate a response and send it back to you.]
[If the available bandwidth between the two points is 10 megabits per second, you'll be able to receive 1.25 megabytes every second.]
[extra 20 percent on top of the total time to download it. But if the content is 10 megabytes or more, the initial latency will be less than five percent of the total time to download it. So it matters less.]
[Remember that if you're transmitting a lot of small pieces of data, you care more about latency than bandwidth.]
[If one connection is transmitting a lot of data, there may be no bandwidth left for the other connections.]
[You can check out which processes are using the network connection by running a program like iftop.]
[You might also have noticed that the more users sharing the same network, the slower the data comes in.]
[If some applications are using so much bandwidth that others can't transmit anymore data, it's possible to restrict how much each connection takes by using traffic shaping.]
[By prioritizing accordingly, processes that send and receive small packets can keep working fine, while processes that need the most bandwidth can use the rest.]
[There's also a limit to how many network connections can be established on a single computer.]
