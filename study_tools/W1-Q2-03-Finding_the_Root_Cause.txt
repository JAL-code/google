[he backup system was blocking the websites from working, and so we mitigated that immediate problem to unblock the user.]
[This could be because the network bandwidth is saturated, the disk transfer is too slow, the hard drive is faulty, or a bunch of other reasons.]
[We also didn't do anything to make sure our backups could run successfully in the future.]
[We generally follow a cycle of looking at the information we have, coming up with a hypothesis that could explain the problem, and then testing our hypothesis.]
[If we confirm our theory, we found the root cause.]
[If we don't, then we go back to the beginning and try different possibility.]
[To get inspired, we look at information we currently have and gather more if we need. Searching online for the error messages that we get or looking at the documentation of the applications involved can also help us imagine new possibilities of what might be at fault.]
[Whenever possible, we should check our hypothesis in a test environment, instead of the production environment that our users are working with.]
[avoid interfering with what our users are doing and we can tinker around without fear of breaking something important.]
[try our code in a newly installed machine, spinning up a test server, using test data, and so on.]
[the extra safety is definitely worth it.]
[it's always a good idea to check if we can reproduce the problem in a test environment before we modify production.]
[On the flip side, if the problem is related to some configuration of either the web services or the backup service, we'd still see it in the test server.]
[start by setting up a test instance of the service and checking if the problem replicates there before touching the production instance.]
[So say we have a test server running the same websites. When we start the backup, we see that the website stop responding.]
[One possible culprit could be too much disk input and output.]
[iotop, which is a tool similar to top that lets us see which processes are using the most input and output.]
[If the issue is that the process generates too much input or output, we could use a command like ionice to make our backup system reduce its priority to access the disk and let the web services use it too.]
[What if the input and output is not the issue?]
[Another option would be that the service is using too much network because it's transmitting the data to be backed up to a central server and that transmission blocks everything else.]
[iftop, yet another tool similar to top that shows the current traffic on the network interfaces.]
[If the backup is eating all the network bandwidth, we could look at the documentation for the backup software and check if it already includes an option to limit the bandwidth.]
[If that option isn't available, we can use a program like Trickle to limit the bandwidth being used.]
[But what if the network isn't the issue either?]
[Another option could be that the compression algorithms selected is too aggressive, and compressing the backups is using all of the server's processing power.]
[solve this by reducing the compression level or using the nice command to reduce the priority of the process and accessing the CPU.]
[If that's still not the case, we need to keep looking, check the logs to see if we find anything that we missed before.]
